# 1. llama-cpp-server Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-cpp-server-deployment
  labels:
    app: llama-cpp-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llama-cpp-server
  template:
    metadata:
      labels:
        app: llama-cpp-server
    spec:
      containers:
      - name: llama-cpp-server
        image: ghcr.io/ggml-org/llama.cpp:server
        # Set MODEL_FILE to the actual model file you have in ./models
        env:
        - name: MODEL_FILE
          value: /models/Qwen3-0.6B-Q8_0.gguf  # <-- Set this to your actual model file
        args:
        - -m
        - /models/Qwen3-0.6B-Q8_0.gguf
        - --port
        - "11434"
        - --host
        - 0.0.0.0
        - -n
        - "1024"
       # --- CORRECTED LIVENESS PROBE ---
        livenessProbe:
          exec:
            # Run a command inside the container to check its own health
            command:
            - /bin/sh
            - -c
            - curl --fail http://localhost:11434/health
          initialDelaySeconds: 300 # Still wait 5 mins for the model to load
          periodSeconds: 15
          timeoutSeconds: 5
          failureThreshold: 3
        volumeMounts:
        - mountPath: /models
          name: llama-cpp-server-data
      volumes:
      - name: llama-cpp-server-data
        hostPath:
          path: ./models
          type: Directory
---
# 2. llama-cpp-server Service
apiVersion: v1
kind: Service
metadata:
  name: llama-cpp-server-service
spec:
  selector:
    app: llama-cpp-server
  ports:
  - protocol: TCP
    port: 11434
    targetPort: 11434
---
# 3. rag-service Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rag-service-deployment
  labels:
    app: rag-service
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rag-service
  template:
    metadata:
      labels:
        app: rag-service
    spec:
      containers:
      - name: rag-service
        image: rag-service:latest
        env:
        - name: LLAMA_CPP_BASE_URL
          value: "http://llama-cpp-server-service:11434"
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: rag-documents
          mountPath: /app/documents
        - name: rag-data
          mountPath: /app/data
        - name: rag-models
          mountPath: /app/models
      volumes:
      - name: rag-documents
        hostPath:
          path: ./rag-service/documents
          type: Directory
      - name: rag-data
        hostPath:
          path: ./rag-service/data
          type: Directory
      - name: rag-models
        hostPath:
          path: ./rag-service/models
          type: Directory
---
# 4. rag-service Service
apiVersion: v1
kind: Service
metadata:
  name: rag-service-service
spec:
  selector:
    app: rag-service
  ports:
  - protocol: TCP
    port: 8080
    targetPort: 8080
---
# 5. apache Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: apache-deployment
  labels:
    app: apache
spec:
  replicas: 1
  selector:
    matchLabels:
      app: apache
  template:
    metadata:
      labels:
        app: apache
    spec:
      containers:
      - name: apache
        image: docker.io/library/httpd:latest
        env:
        - name: LLAMA_CPP_HOST
          value: "llama-cpp-server-service:11434"
        - name: APACHE_LOG_DIR
          value: /usr/local/apache2/logs
        ports:
        - containerPort: 80
          hostPort: 8080
        - containerPort: 443
          hostPort: 8443
        volumeMounts:
        - name: apache-conf
          mountPath: /usr/local/apache2/conf
        - name: apache-certs
          mountPath: /usr/local/apache2/certs
        - name: apache-logs
          mountPath: /usr/local/apache2/logs
        - name: apache-htdocs
          mountPath: /var/www/html
      volumes:
      - name: apache-conf
        hostPath:
          path: ./apache/conf
          type: Directory
      - name: apache-certs
        hostPath:
          path: ./apache/certs
          type: Directory
      - name: apache-logs
        hostPath:
          path: ./apache/logs
          type: Directory
      - name: apache-htdocs
        hostPath:
          path: ./apache/html
          type: Directory
---
# 4. NetworkPolicy
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: llama-cpp-server-network-policy
spec:
  podSelector:
    matchLabels:
      app: llama-cpp-server
  policyTypes:
    - Ingress
    - Egress
  # Ingress policy
  ingress:
    - from:
      - podSelector:
          matchLabels:
            app: apache
      ports:
      - protocol: TCP
        port: 11434
  # Egress policy
  egress:
    # For DNS resolve
    - to:
      - namespaceSelector: {}
      ports:
      - protocol: UDP
        port: 53
      - protocol: TCP
        port: 53
    # For apache Pod
    - to:
      - podSelector:
          matchLabels:
            app: apache
