# 0. ConfigMap for non-secret environment variables
#
# Policy (keep it simple):
# - Shared, non-sensitive settings live in this ConfigMap and are injected via envFrom.
# - Secrets (passwords/tokens) live in a Secret and are injected via env.
# - Pod-specific one-off settings can stay in the individual Deployment when it improves clarity.
apiVersion: v1
kind: ConfigMap
metadata:
  name: model-config
data:
  # Model Configuration
  MODEL_DIRECTORY: /models
  EMBEDDING_MODEL_NAME: Qwen3-Embedding-0.6B-Q8_0
  RERANK_MODEL_NAME: qwen3-reranker-0.6b-q8_0
  # RAG Service Configuration (non-secret)
  # Podman play kube resolves the pod/deployment name (e.g. 'llama-cpp-server'),
  # but not the Kubernetes Service name 'llama-cpp-server-service'.
  LLAMA_CPP_BASE_URL: "http://llama-cpp-server:11434"
  # Use the model id returned by llama.cpp /v1/models (not the .gguf filename).
  LLAMA_CPP_MODEL_NAME: "Qwen3-0.6B-Q8_0"
  EMBEDDING_API_BASE: "http://embedding-service:8080/v1"
  RERANK_API_BASE: "http://rerank-service:8080/v1"
  RETRIEVAL_CANDIDATES: "50"
  RERANK_TOP_K: "5"
  # MCP Bridge Configuration (non-secret)
  LLM_BASE_URL: "http://llama-cpp-server:11434/v1"
  LLM_MODEL: "Qwen3-0.6B-Q8_0"
  # For local llama.cpp OpenAI-compatible server, any non-empty key is fine.
  # If you later use a real remote provider, move this to a Secret.
  OPENAI_API_KEY: "local"
  PG_HOST: "postgresql"
  PG_PORT: "5432"
  PG_DATABASE: "test"
  # File Upload Security Limits
  MAX_FILE_SIZE_MB: "10"
  MAX_UPLOAD_SIZE_MB: "50"
  MAX_FILES_PER_UPLOAD: "10"
  # Path Security
  ALLOWED_DOCUMENT_PATHS: /app/documents,/tmp/rag_uploads
  # Logging
  LOG_LEVEL: INFO
---
# 0.5 Secret for PostgreSQL credentials (local development defaults)
# For production: create this Secret separately and DO NOT commit real passwords.
# Example: kubectl create secret generic postgresql-credentials --from-literal=...
apiVersion: v1
kind: Secret
metadata:
  name: postgresql-credentials
type: Opaque
stringData:
  # PostgreSQL superuser password (used by postgres container)
  POSTGRES_PASSWORD: "postgres"
  # Application user credentials (created by init.sql)
  PG_USER: "appuser"
  PG_PASSWORD: "apppassword"
---
# 1. llama-cpp-server Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-cpp-server-deployment
  labels:
    app: llama-cpp-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llama-cpp-server
  template:
    metadata:
      labels:
        app: llama-cpp-server
    spec:
      containers:
      - name: llama-cpp-server
        image: ghcr.io/ggml-org/llama.cpp:server
        envFrom:
        - configMapRef:
            name: model-config
        args:
        - --models-dir
        - $(MODEL_DIRECTORY)
        - --port
        - "11434"
        - --host
        - 0.0.0.0
        - -n
        - "4096"
        - -c
        - "8192"
        - -np
        - "4"
        - -sps
        - "0.5"
        - --cache-reuse
        - "128"
       # --- CORRECTED LIVENESS PROBE ---
        livenessProbe:
          exec:
            # Run a command inside the container to check its own health
            command:
            - /bin/sh
            - -c
            - curl --fail http://localhost:11434/health
          initialDelaySeconds: 300 # Still wait 5 mins for the model to load
          periodSeconds: 15
          timeoutSeconds: 5
          failureThreshold: 3
        volumeMounts:
        - mountPath: /models
          name: llama-cpp-server-data
      volumes:
      - name: llama-cpp-server-data
        hostPath:
          path: ./models
          type: Directory
---
# 2. llama-cpp-server Service
apiVersion: v1
kind: Service
metadata:
  name: llama-cpp-server-service
spec:
  selector:
    app: llama-cpp-server
  ports:
  - protocol: TCP
    port: 11434
    targetPort: 11434
---
# 3. rag-service Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rag-service-deployment
  labels:
    app: rag-service
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rag-service
  template:
    metadata:
      labels:
        app: rag-service
    spec:
      containers:
      - name: rag-service
        image: rag-service:latest
        envFrom:
        - configMapRef:
            name: model-config
        ports:
        - containerPort: 8080
#        resources:
#          limits:
#            memory: "4Gi"
#            cpu: "2000m"
#          requests:
#            memory: "2Gi"
#            cpu: "1000m"
#        securityContext:
#          runAsNonRoot: true
#          runAsUser: 1000
#          allowPrivilegeEscalation: false
        volumeMounts:
        - name: rag-documents
          mountPath: /app/documents
        - name: rag-data
          mountPath: /app/data
        - name: rag-models
          mountPath: /app/models
      volumes:
      - name: rag-documents
        hostPath:
          path: ./rag-service/documents
          type: Directory
      - name: rag-data
        hostPath:
          path: ./rag-service/data
          type: Directory
      - name: rag-models
        hostPath:
          path: ./rag-service/models
          type: Directory
---
# 4. rag-service Service
apiVersion: v1
kind: Service
metadata:
  name: rag-service-service
spec:
  selector:
    app: rag-service
  ports:
  - protocol: TCP
    port: 8080
    targetPort: 8080

---
# 5. embedding-service Deployment (llama.cpp --embedding)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: embedding-service-deployment
  labels:
    app: embedding-service
spec:
  replicas: 1
  selector:
    matchLabels:
      app: embedding-service
  template:
    metadata:
      labels:
        app: embedding-service
    spec:
      containers:
      - name: embedding-service
        image: ghcr.io/ggml-org/llama.cpp:server
        envFrom:
        - configMapRef:
            name: model-config
        args:
        - --model
        - /models/$(EMBEDDING_MODEL_NAME).gguf # TODO: replace with actual embedding GGUF filename
        - --embedding
        - --port
        - "8080"
        - --host
        - 0.0.0.0
        ports:
        - containerPort: 8080
        volumeMounts:
        - mountPath: /models
          name: shared-models
      volumes:
      - name: shared-models
        hostPath:
          path: ./rag-service/models/embedding
          type: Directory

---
# 6. embedding-service Service
apiVersion: v1
kind: Service
metadata:
  name: embedding-service
spec:
  selector:
    app: embedding-service
  ports:
  - protocol: TCP
    port: 8080
    targetPort: 8080

---
# 7. rerank-service Deployment (llama.cpp --reranking)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rerank-service-deployment
  labels:
    app: rerank-service
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rerank-service
  template:
    metadata:
      labels:
        app: rerank-service
    spec:
      containers:
      - name: rerank-service
        image: ghcr.io/ggml-org/llama.cpp:server
        envFrom:
        - configMapRef:
            name: model-config
        args:
        - --model
        - /models/$(RERANK_MODEL_NAME).gguf # TODO: replace with actual reranker GGUF filename
        - --reranking
        - --port
        - "8080"
        - --host
        - 0.0.0.0
        ports:
        - containerPort: 8080
        volumeMounts:
        - mountPath: /models
          name: shared-models
      volumes:
      - name: shared-models
        hostPath:
          path: ./rag-service/models/reranker
          type: Directory

---
# 8. rerank-service Service
apiVersion: v1
kind: Service
metadata:
  name: rerank-service
spec:
  selector:
    app: rerank-service
  ports:
  - protocol: TCP
    port: 8080
    targetPort: 8080

---
# 8.5 postgresql Deployment (optional: for MCP DB tools)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgresql-deployment
  labels:
    app: postgresql
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgresql
  template:
    metadata:
      labels:
        app: postgresql
    spec:
      containers:
      - name: postgresql
        image: docker.io/library/postgres:18
        env:
        - name: POSTGRES_DB
          value: "test"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgresql-credentials
              key: POSTGRES_PASSWORD
        resources:
          limits:
            memory: "512Mi"
            cpu: "500m"
          requests:
            memory: "256Mi"
            cpu: "100m"
        ports:
        - containerPort: 5432
        volumeMounts:
        # Postgres runs scripts in /docker-entrypoint-initdb.d only on first init
        # (i.e., when the data directory is empty). If you need to re-apply
        # init.sql, delete the hostPath directory mounted at postgresql-data.
        - name: postgresql-init-sql
          mountPath: /docker-entrypoint-initdb.d/00-init.sql
        - name: postgresql-data
          # Postgres 18+ stores PGDATA in a major-version-specific subdirectory.
          # The recommended configuration is to mount /var/lib/postgresql (not /var/lib/postgresql/data).
          mountPath: /var/lib/postgresql
      volumes:
      - name: postgresql-init-sql
        hostPath:
          path: ./mcp-script/postgresql/init.sql
          type: File
      - name: postgresql-data
        hostPath:
          # Separate from older layouts that mounted /var/lib/postgresql/data.
          path: ./mcp-script/postgresql-data
          type: DirectoryOrCreate

---
# 8.6 postgresql Service
apiVersion: v1
kind: Service
metadata:
  name: postgresql
spec:
  selector:
    app: postgresql
  ports:
  - protocol: TCP
    port: 5432
    targetPort: 5432

---
# 8.7 MCP bridge Deployment (interactive; exec into pod for TTY)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mcp-bridge-deployment
  labels:
    app: mcp-bridge
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mcp-bridge
  template:
    metadata:
      labels:
        app: mcp-bridge
    spec:
      containers:
      - name: mcp-bridge
        image: mcp-bridge:latest
        stdin: true
        tty: true
        ports:
        - containerPort: 8090
        resources:
          limits:
            memory: "512Mi"
            cpu: "500m"
          requests:
            memory: "128Mi"
            cpu: "100m"
        envFrom:
        - configMapRef:
            name: model-config
        env:
        - name: PG_USER
          valueFrom:
            secretKeyRef:
              name: postgresql-credentials
              key: PG_USER
        - name: PG_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgresql-credentials
              key: PG_PASSWORD
---
# 9. apache Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: apache-deployment
  labels:
    app: apache
spec:
  replicas: 1
  selector:
    matchLabels:
      app: apache
  template:
    metadata:
      labels:
        app: apache
    spec:
      containers:
      - name: apache
        image: docker.io/library/httpd:latest
        env:
        - name: LLAMA_CPP_HOST
          value: "llama-cpp-server-service:11434"
        - name: APACHE_LOG_DIR
          value: /usr/local/apache2/logs
        ports:
        - containerPort: 80
          hostPort: 8080
        - containerPort: 443
          hostPort: 8443
        volumeMounts:
        - name: apache-conf
          mountPath: /usr/local/apache2/conf
        - name: apache-htpasswd
          mountPath: /usr/local/apache2/.htpasswd
        - name: apache-certs
          mountPath: /usr/local/apache2/certs
        - name: apache-logs
          mountPath: /usr/local/apache2/logs
        - name: apache-htdocs
          mountPath: /var/www/html
      volumes:
      - name: apache-conf
        hostPath:
          path: ./apache/conf
          type: Directory
      - name: apache-htpasswd
        hostPath:
          path: ./apache/.htpasswd
          type: File
      - name: apache-certs
        hostPath:
          path: ./apache/certs
          type: Directory
      - name: apache-logs
        hostPath:
          path: ./apache/logs
          type: Directory
      - name: apache-htdocs
        hostPath:
          path: ./apache/html
          type: Directory
---
    # 10. NetworkPolicy: llama-cpp-server ingress/egress
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: llama-cpp-server-network-policy
spec:
  podSelector:
    matchLabels:
      app: llama-cpp-server
  policyTypes:
    - Ingress
    - Egress
  # Ingress policy
  ingress:
    - from:
      - podSelector:
          matchLabels:
            app: apache
      - podSelector:
          matchLabels:
            app: rag-service
      - podSelector:
          matchLabels:
            app: mcp-bridge
      ports:
      - protocol: TCP
        port: 11434
  # Egress policy
  egress:
    # For DNS resolve
    - to:
      - namespaceSelector: {}
      ports:
      - protocol: UDP
        port: 53
      - protocol: TCP
        port: 53
    # For apache Pod
    - to:
      - podSelector:
          matchLabels:
            app: apache

---
# 11. NetworkPolicy: embedding-service ingress from rag-service only
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: embedding-service-network-policy
spec:
  podSelector:
    matchLabels:
      app: embedding-service
  policyTypes:
    - Ingress
  ingress:
    - from:
      - podSelector:
          matchLabels:
            app: rag-service
      ports:
      - protocol: TCP
        port: 8080

---
# 12. NetworkPolicy: rerank-service ingress from rag-service only
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: rerank-service-network-policy
spec:
  podSelector:
    matchLabels:
      app: rerank-service
  policyTypes:
    - Ingress
  ingress:
    - from:
      - podSelector:
          matchLabels:
            app: rag-service
      ports:
      - protocol: TCP
        port: 8080

---
# 13. NetworkPolicy: rag-service egress only to LLM, embedding, rerank, DNS
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: rag-service-egress-policy
spec:
  podSelector:
    matchLabels:
      app: rag-service
  policyTypes:
    - Egress
  egress:
    # DNS
    - to:
      - namespaceSelector: {}
      ports:
      - protocol: UDP
        port: 53
      - protocol: TCP
        port: 53
    # llama-cpp-server
    - to:
      - podSelector:
          matchLabels:
            app: llama-cpp-server
      ports:
      - protocol: TCP
        port: 11434
    # embedding-service
    - to:
      - podSelector:
          matchLabels:
            app: embedding-service
      ports:
      - protocol: TCP
        port: 8080
    # rerank-service
    - to:
      - podSelector:
          matchLabels:
            app: rerank-service
      ports:
      - protocol: TCP
        port: 8080
