# 0. ConfigMap for environment variables (sync with config/.env)
apiVersion: v1
kind: ConfigMap
metadata:
  name: model-config
data:
  # Model Configuration
  MODEL_DIRECTORY: /models
  EMBEDDING_MODEL_NAME: Qwen3-Embedding-0.6B-Q8_0
  RERANK_MODEL_NAME: qwen3-reranker-0.6b-q8_0
  # File Upload Security Limits
  MAX_FILE_SIZE_MB: "10"
  MAX_UPLOAD_SIZE_MB: "50"
  MAX_FILES_PER_UPLOAD: "10"
  # Path Security
  ALLOWED_DOCUMENT_PATHS: /app/documents,/tmp/rag_uploads
  # Logging
  LOG_LEVEL: INFO
---
# 1. llama-cpp-server Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-cpp-server-deployment
  labels:
    app: llama-cpp-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llama-cpp-server
  template:
    metadata:
      labels:
        app: llama-cpp-server
    spec:
      containers:
      - name: llama-cpp-server
        image: ghcr.io/ggml-org/llama.cpp:server
        envFrom:
        - configMapRef:
            name: model-config
        args:
        - --models-dir
        - $(MODEL_DIRECTORY)
        - --port
        - "11434"
        - --host
        - 0.0.0.0
        - -n
        - "4096"
        - -c
        - "8192"
        - -np
        - "4"
        - -sps
        - "0.5"
        - --cache-reuse
        - "128"
       # --- CORRECTED LIVENESS PROBE ---
        livenessProbe:
          exec:
            # Run a command inside the container to check its own health
            command:
            - /bin/sh
            - -c
            - curl --fail http://localhost:11434/health
          initialDelaySeconds: 300 # Still wait 5 mins for the model to load
          periodSeconds: 15
          timeoutSeconds: 5
          failureThreshold: 3
        volumeMounts:
        - mountPath: /models
          name: llama-cpp-server-data
      volumes:
      - name: llama-cpp-server-data
        hostPath:
          path: ./models
          type: Directory
---
# 2. llama-cpp-server Service
apiVersion: v1
kind: Service
metadata:
  name: llama-cpp-server-service
spec:
  selector:
    app: llama-cpp-server
  ports:
  - protocol: TCP
    port: 11434
    targetPort: 11434
---
# 3. rag-service Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rag-service-deployment
  labels:
    app: rag-service
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rag-service
  template:
    metadata:
      labels:
        app: rag-service
    spec:
      containers:
      - name: rag-service
        image: rag-service:latest
        env:
        - name: LLAMA_CPP_BASE_URL
          # Podman play kube resolves the pod/deployment name (e.g. 'llama-cpp-server'),
          # but not the Kubernetes Service name 'llama-cpp-server-service'.
          value: "http://llama-cpp-server:11434"
        - name: LLAMA_CPP_MODEL_NAME
          # Use the model id returned by llama.cpp /v1/models (not the .gguf filename).
          value: "Qwen3-0.6B-Q8_0"
        - name: EMBEDDING_API_BASE
          value: "http://embedding-service:8080/v1"
        - name: RERANK_API_BASE
          value: "http://rerank-service:8080/v1"
        - name: RETRIEVAL_CANDIDATES
          value: "50"
        - name: RERANK_TOP_K
          value: "5"
        envFrom:
        - configMapRef:
            name: model-config
        ports:
        - containerPort: 8080
#        resources:
#          limits:
#            memory: "4Gi"
#            cpu: "2000m"
#          requests:
#            memory: "2Gi"
#            cpu: "1000m"
#        securityContext:
#          runAsNonRoot: true
#          runAsUser: 1000
#          allowPrivilegeEscalation: false
        volumeMounts:
        - name: rag-documents
          mountPath: /app/documents
        - name: rag-data
          mountPath: /app/data
        - name: rag-models
          mountPath: /app/models
      volumes:
      - name: rag-documents
        hostPath:
          path: ./rag-service/documents
          type: Directory
      - name: rag-data
        hostPath:
          path: ./rag-service/data
          type: Directory
      - name: rag-models
        hostPath:
          path: ./rag-service/models
          type: Directory
---
# 4. rag-service Service
apiVersion: v1
kind: Service
metadata:
  name: rag-service-service
spec:
  selector:
    app: rag-service
  ports:
  - protocol: TCP
    port: 8080
    targetPort: 8080

---
# 5. embedding-service Deployment (llama.cpp --embedding)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: embedding-service-deployment
  labels:
    app: embedding-service
spec:
  replicas: 1
  selector:
    matchLabels:
      app: embedding-service
  template:
    metadata:
      labels:
        app: embedding-service
    spec:
      containers:
      - name: embedding-service
        image: ghcr.io/ggml-org/llama.cpp:server
        envFrom:
        - configMapRef:
            name: model-config
        args:
        - --model
        - /models/$(EMBEDDING_MODEL_NAME).gguf # TODO: replace with actual embedding GGUF filename
        - --embedding
        - --port
        - "8080"
        - --host
        - 0.0.0.0
        ports:
        - containerPort: 8080
        volumeMounts:
        - mountPath: /models
          name: shared-models
      volumes:
      - name: shared-models
        hostPath:
          path: ./rag-service/models/embedding
          type: Directory

---
# 6. embedding-service Service
apiVersion: v1
kind: Service
metadata:
  name: embedding-service
spec:
  selector:
    app: embedding-service
  ports:
  - protocol: TCP
    port: 8080
    targetPort: 8080

---
# 7. rerank-service Deployment (llama.cpp --reranking)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rerank-service-deployment
  labels:
    app: rerank-service
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rerank-service
  template:
    metadata:
      labels:
        app: rerank-service
    spec:
      containers:
      - name: rerank-service
        image: ghcr.io/ggml-org/llama.cpp:server
        envFrom:
        - configMapRef:
            name: model-config
        args:
        - --model
        - /models/$(RERANK_MODEL_NAME).gguf # TODO: replace with actual reranker GGUF filename
        - --reranking
        - --port
        - "8080"
        - --host
        - 0.0.0.0
        ports:
        - containerPort: 8080
        volumeMounts:
        - mountPath: /models
          name: shared-models
      volumes:
      - name: shared-models
        hostPath:
          path: ./rag-service/models/reranker
          type: Directory

---
# 8. rerank-service Service
apiVersion: v1
kind: Service
metadata:
  name: rerank-service
spec:
  selector:
    app: rerank-service
  ports:
  - protocol: TCP
    port: 8080
    targetPort: 8080
---
# 9. apache Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: apache-deployment
  labels:
    app: apache
spec:
  replicas: 1
  selector:
    matchLabels:
      app: apache
  template:
    metadata:
      labels:
        app: apache
    spec:
      containers:
      - name: apache
        image: docker.io/library/httpd:latest
        env:
        - name: LLAMA_CPP_HOST
          value: "llama-cpp-server-service:11434"
        - name: APACHE_LOG_DIR
          value: /usr/local/apache2/logs
        ports:
        - containerPort: 80
          hostPort: 8080
        - containerPort: 443
          hostPort: 8443
        volumeMounts:
        - name: apache-conf
          mountPath: /usr/local/apache2/conf
        - name: apache-htpasswd
          mountPath: /usr/local/apache2/.htpasswd
        - name: apache-certs
          mountPath: /usr/local/apache2/certs
        - name: apache-logs
          mountPath: /usr/local/apache2/logs
        - name: apache-htdocs
          mountPath: /var/www/html
      volumes:
      - name: apache-conf
        hostPath:
          path: ./apache/conf
          type: Directory
      - name: apache-htpasswd
        hostPath:
          path: ./apache/.htpasswd
          type: File
      - name: apache-certs
        hostPath:
          path: ./apache/certs
          type: Directory
      - name: apache-logs
        hostPath:
          path: ./apache/logs
          type: Directory
      - name: apache-htdocs
        hostPath:
          path: ./apache/html
          type: Directory
---
    # 10. NetworkPolicy: llama-cpp-server ingress/egress
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: llama-cpp-server-network-policy
spec:
  podSelector:
    matchLabels:
      app: llama-cpp-server
  policyTypes:
    - Ingress
    - Egress
  # Ingress policy
  ingress:
    - from:
      - podSelector:
          matchLabels:
            app: apache
      ports:
      - protocol: TCP
        port: 11434
  # Egress policy
  egress:
    # For DNS resolve
    - to:
      - namespaceSelector: {}
      ports:
      - protocol: UDP
        port: 53
      - protocol: TCP
        port: 53
    # For apache Pod
    - to:
      - podSelector:
          matchLabels:
            app: apache

---
# 11. NetworkPolicy: embedding-service ingress from rag-service only
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: embedding-service-network-policy
spec:
  podSelector:
    matchLabels:
      app: embedding-service
  policyTypes:
    - Ingress
  ingress:
    - from:
      - podSelector:
          matchLabels:
            app: rag-service
      ports:
      - protocol: TCP
        port: 8080

---
# 12. NetworkPolicy: rerank-service ingress from rag-service only
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: rerank-service-network-policy
spec:
  podSelector:
    matchLabels:
      app: rerank-service
  policyTypes:
    - Ingress
  ingress:
    - from:
      - podSelector:
          matchLabels:
            app: rag-service
      ports:
      - protocol: TCP
        port: 8080

---
# 13. NetworkPolicy: rag-service egress only to LLM, embedding, rerank, DNS
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: rag-service-egress-policy
spec:
  podSelector:
    matchLabels:
      app: rag-service
  policyTypes:
    - Egress
  egress:
    # DNS
    - to:
      - namespaceSelector: {}
      ports:
      - protocol: UDP
        port: 53
      - protocol: TCP
        port: 53
    # llama-cpp-server
    - to:
      - podSelector:
          matchLabels:
            app: llama-cpp-server
      ports:
      - protocol: TCP
        port: 11434
    # embedding-service
    - to:
      - podSelector:
          matchLabels:
            app: embedding-service
      ports:
      - protocol: TCP
        port: 8080
    # rerank-service
    - to:
      - podSelector:
          matchLabels:
            app: rerank-service
      ports:
      - protocol: TCP
        port: 8080
