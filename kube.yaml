# 0. ConfigMap for environment variables (sync with config/.env)
apiVersion: v1
kind: ConfigMap
metadata:
  name: model-config
data:
  # Model Configuration
  MODEL_FILE: /models/Qwen3-0.6B-Q8_0.gguf
  EMBEDDING_MODEL_PATH: /app/models/embedding/Qwen3-Embedding-0.6B
  RERANKER_MODEL_PATH: /app/models/reranker/bge-reranker-v2-m3
  # File Upload Security Limits
  MAX_FILE_SIZE_MB: "10"
  MAX_UPLOAD_SIZE_MB: "50"
  MAX_FILES_PER_UPLOAD: "10"
  # Path Security
  ALLOWED_DOCUMENT_PATHS: /app/documents,/tmp/rag_uploads
  # Logging
  LOG_LEVEL: INFO
---
# 1. llama-cpp-server Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-cpp-server-deployment
  labels:
    app: llama-cpp-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llama-cpp-server
  template:
    metadata:
      labels:
        app: llama-cpp-server
    spec:
      containers:
      - name: llama-cpp-server
        image: ghcr.io/ggml-org/llama.cpp:server
        envFrom:
        - configMapRef:
            name: model-config
        args:
        - -m
        - $(MODEL_FILE)
        - --port
        - "11434"
        - --host
        - 0.0.0.0
        - -n
        - "4096"
       # --- CORRECTED LIVENESS PROBE ---
        livenessProbe:
          exec:
            # Run a command inside the container to check its own health
            command:
            - /bin/sh
            - -c
            - curl --fail http://localhost:11434/health
          initialDelaySeconds: 300 # Still wait 5 mins for the model to load
          periodSeconds: 15
          timeoutSeconds: 5
          failureThreshold: 3
        volumeMounts:
        - mountPath: /models
          name: llama-cpp-server-data
      volumes:
      - name: llama-cpp-server-data
        hostPath:
          path: ./models
          type: Directory
---
# 2. llama-cpp-server Service
apiVersion: v1
kind: Service
metadata:
  name: llama-cpp-server-service
spec:
  selector:
    app: llama-cpp-server
  ports:
  - protocol: TCP
    port: 11434
    targetPort: 11434
---
# 3. rag-service Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rag-service-deployment
  labels:
    app: rag-service
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rag-service
  template:
    metadata:
      labels:
        app: rag-service
    spec:
      containers:
      - name: rag-service
        image: rag-service:latest
        env:
        - name: LLAMA_CPP_BASE_URL
          value: "http://llama-cpp-server-service:11434"
        envFrom:
        - configMapRef:
            name: model-config
        ports:
        - containerPort: 8080
#        resources:
#          limits:
#            memory: "4Gi"
#            cpu: "2000m"
#          requests:
#            memory: "2Gi"
#            cpu: "1000m"
#        securityContext:
#          runAsNonRoot: true
#          runAsUser: 1000
#          allowPrivilegeEscalation: false
        volumeMounts:
        - name: rag-documents
          mountPath: /app/documents
        - name: rag-data
          mountPath: /app/data
        - name: rag-models
          mountPath: /app/models
      volumes:
      - name: rag-documents
        hostPath:
          path: ./rag-service/documents
          type: Directory
      - name: rag-data
        hostPath:
          path: ./rag-service/data
          type: Directory
      - name: rag-models
        hostPath:
          path: ./rag-service/models
          type: Directory
---
# 4. rag-service Service
apiVersion: v1
kind: Service
metadata:
  name: rag-service-service
spec:
  selector:
    app: rag-service
  ports:
  - protocol: TCP
    port: 8080
    targetPort: 8080
---
# 5. apache Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: apache-deployment
  labels:
    app: apache
spec:
  replicas: 1
  selector:
    matchLabels:
      app: apache
  template:
    metadata:
      labels:
        app: apache
    spec:
      containers:
      - name: apache
        image: docker.io/library/httpd:latest
        env:
        - name: LLAMA_CPP_HOST
          value: "llama-cpp-server-service:11434"
        - name: APACHE_LOG_DIR
          value: /usr/local/apache2/logs
        ports:
        - containerPort: 80
          hostPort: 8080
        - containerPort: 443
          hostPort: 8443
        volumeMounts:
        - name: apache-conf
          mountPath: /usr/local/apache2/conf
        - name: apache-certs
          mountPath: /usr/local/apache2/certs
        - name: apache-logs
          mountPath: /usr/local/apache2/logs
        - name: apache-htdocs
          mountPath: /var/www/html
      volumes:
      - name: apache-conf
        hostPath:
          path: ./apache/conf
          type: Directory
      - name: apache-certs
        hostPath:
          path: ./apache/certs
          type: Directory
      - name: apache-logs
        hostPath:
          path: ./apache/logs
          type: Directory
      - name: apache-htdocs
        hostPath:
          path: ./apache/html
          type: Directory
---
# 4. NetworkPolicy
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: llama-cpp-server-network-policy
spec:
  podSelector:
    matchLabels:
      app: llama-cpp-server
  policyTypes:
    - Ingress
    - Egress
  # Ingress policy
  ingress:
    - from:
      - podSelector:
          matchLabels:
            app: apache
      ports:
      - protocol: TCP
        port: 11434
  # Egress policy
  egress:
    # For DNS resolve
    - to:
      - namespaceSelector: {}
      ports:
      - protocol: UDP
        port: 53
      - protocol: TCP
        port: 53
    # For apache Pod
    - to:
      - podSelector:
          matchLabels:
            app: apache
